{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFXO79neFUmn"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8b197nUEJYu"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataPreprocessorLogisticReg:\n",
    "    def __init__(self, file_path, mappings_file):\n",
    "        self.file_path = file_path\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        print(\"✔ Loaded dataset successfully.\")\n",
    "\n",
    "        # Load mappings from the provided JSON file\n",
    "        with fs.open(mappings_file, 'r') as f:\n",
    "            mappings = json.load(f)\n",
    "            self.product_category_indices = mappings['product_category_indices']\n",
    "            self.hazard_category_indices = mappings['hazard_category_indices']\n",
    "            self.product_indices = mappings['product_indices']\n",
    "            self.hazard_indices = mappings['hazard_indices']\n",
    "\n",
    "        # Initialize reverse mappings\n",
    "        self.product_category_labels = {v: k for k, v in self.product_category_indices.items()}\n",
    "        self.hazard_category_labels = {v: k for k, v in self.hazard_category_indices.items()}\n",
    "        self.product_labels = {v: k for k, v in self.product_indices.items()}\n",
    "        self.hazard_labels = {v: k for k, v in self.hazard_indices.items()}\n",
    "\n",
    "        # Initialize the Lemmatizer and Stop Words\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        self.product_data = self.data[['title', 'product']]\n",
    "        self.hazard_data = self.data[['title', 'hazard']]\n",
    "        self.product_category_data = self.data[['title', 'product-category']]\n",
    "        self.hazard_category_data = self.data[['title', 'hazard-category']]\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        return BeautifulSoup(text, \"html.parser\").get_text() if text else \"\"\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        text = ' '.join(\n",
    "            [self.lemmatizer.lemmatize(word) for word in text.split() if\n",
    "             word not in self.stop_words]\n",
    "        )\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def combine_date_columns(self):\n",
    "        if {'year', 'month', 'day'}.issubset(self.data.columns):\n",
    "            self.data['date'] = pd.to_datetime(\n",
    "                self.data[['year', 'month', 'day']])\n",
    "            self.data['date'] = self.data['date'].astype('int64') // 1e9\n",
    "            self.data.drop(['year', 'month', 'day'], axis=1, inplace=True)\n",
    "        print(\"✔ Combined date columns successfully.\")\n",
    "\n",
    "    def encode_country_column(self):\n",
    "        if 'country' in self.data.columns:\n",
    "            # Save the mapping of the numerical index to the country name\n",
    "            self.country_mapping = dict(\n",
    "                enumerate(self.data['country'].factorize()[1]))\n",
    "            self.data['country'] = pd.factorize(self.data['country'])[0]\n",
    "        print(\"✔ Encoded country column successfully.\")\n",
    "\n",
    "    def vectorize_data(self):\n",
    "        # Vectorizer instance\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "        # List to store the updated datasets with TF-IDF vectors\n",
    "        updated_datasets = []\n",
    "\n",
    "        for dataset in [self.product_data, self.hazard_data, self.product_category_data, self.hazard_category_data]:\n",
    "            # Fit and transform the title data\n",
    "            tfidf_matrix = vectorizer.fit_transform(dataset['title'])\n",
    "\n",
    "            # Convert the TF-IDF matrix to a DataFrame\n",
    "            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])])\n",
    "\n",
    "            # Reset index of TF-IDF DataFrame to align with the dataset\n",
    "            tfidf_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Concatenate the original dataset with the TF-IDF DataFrame\n",
    "            dataset_with_tfidf = pd.concat([dataset.reset_index(drop=True), tfidf_df], axis=1)\n",
    "\n",
    "            # Append the updated dataset\n",
    "            updated_datasets.append(dataset_with_tfidf)\n",
    "\n",
    "        # Assign the updated datasets back to their respective variables\n",
    "        self.product_data, self.hazard_data, self.product_category_data, self.hazard_category_data = updated_datasets\n",
    "        print(\"✔ Vectorized data using TF-IDF and added columns successfully.\")\n",
    "\n",
    "    def generate_synthetic_data(self, min_samples = 2):\n",
    "        # create llm pipeline:\n",
    "        # Get special tokens for later:\n",
    "        # Use default special tokens if custom tokens are not in the tokenizer's vocabulary\n",
    "        bos_token_id = llm.tokenizer.bos_token_id\n",
    "        eos_token_id = llm.tokenizer.eos_token_id\n",
    "        pad_token_id = llm.tokenizer.pad_token_id\n",
    "\n",
    "        terminators = [\n",
    "            llm.tokenizer.eos_token_id,\n",
    "            llm.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "\n",
    "        self.seperated_dataset = [self.product_data, self.hazard_data, self.product_category_data, self.hazard_category_data]\n",
    "\n",
    "        updated_datasets = []  # To store the updated datasets\n",
    "\n",
    "        for i, spec_data in enumerate(self.seperated_dataset):\n",
    "            print(f\"Number {i}\")\n",
    "\n",
    "            balanced_data = []\n",
    "            column = spec_data.columns[1]\n",
    "            print(column)\n",
    "            # Find rare classes\n",
    "            class_counts = spec_data[column].value_counts()\n",
    "            rare_classes = class_counts[class_counts < min_samples].index\n",
    "\n",
    "            for rare_class in tqdm(rare_classes, desc=\"Processing rare classes\"):\n",
    "                # Get data points belonging to the rare class\n",
    "                rare_class_data = spec_data[spec_data[column] == rare_class]\n",
    "                # Generate synthetic samples for the rare class\n",
    "                target_count = min_samples - len(rare_class_data)\n",
    "                num_created_data = target_count\n",
    "                # Use one representative prompt to generate the required number of samples\n",
    "                if len(rare_class_data) > 0:\n",
    "                    representative_row = rare_class_data.iloc[0]  # Use the first row as a representative example\n",
    "                    prompt = [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a syntethic data generator\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"You are given the title of a Food Incident Report, labeled as {rare_class}. Create a single concise title for a different food incident report that captures a similar meaning to the input. Only provide the new title as the output. No explanations or examples.\\nInput: {representative_row['title']}\\nOutput:\"},\n",
    "                    ]\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                      synthetic_samples = llm(\n",
    "                          prompt,\n",
    "                          max_new_tokens=25,\n",
    "                          do_sample=True,\n",
    "                          temperature=0.7,  # Adjusts randomness; lower values make output more deterministic\n",
    "                          top_p=0.9,\n",
    "                          num_return_sequences=num_created_data,\n",
    "                          pad_token_id=llm.tokenizer.eos_token_id,\n",
    "                          bos_token_id=bos_token_id,\n",
    "                          eos_token_id=terminators\n",
    "                      )\n",
    "\n",
    "                    # Extracting the 'assistant' responses\n",
    "                    assistant_responses = []\n",
    "                    for response in synthetic_samples:\n",
    "                        for message in response['generated_text']:\n",
    "                            if message['role'] == 'assistant':\n",
    "                                assistant_responses.append(message['content'])\n",
    "\n",
    "                    for assistant_response in assistant_responses:\n",
    "                        new_row = representative_row.copy()\n",
    "                        new_row['title'] = assistant_response  # Update the text with the generated output\n",
    "                        balanced_data.append(new_row)\n",
    "                else:\n",
    "                    print(f\"No data points found for rare class (this shouldn't be happening): {rare_class}\")\n",
    "                del synthetic_samples, assistant_responses, new_row\n",
    "                gc.collect()\n",
    "            # Add the synthetic data to the original dataset\n",
    "            if balanced_data:\n",
    "                balanced_df = pd.DataFrame(balanced_data)\n",
    "                spec_data = pd.concat([spec_data, balanced_df], ignore_index=True)\n",
    "\n",
    "            updated_datasets.append(spec_data)\n",
    "\n",
    "        self.product_data, self.hazard_data, self.product_category_data, self.hazard_category_data = updated_datasets\n",
    "        print(\"✔ Balanced data using LLM-generated synthetic data.\")\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.generate_synthetic_data(min_samples=10)\n",
    "        \"\"\"self.product_data['title'] = self.product_data['title'].fillna('').apply(self.preprocess_text)\n",
    "        self.hazard_data['title'] = self.hazard_data['title'].fillna('').apply(self.preprocess_text)\n",
    "        self.product_category_data['title'] = self.product_category_data['title'].fillna('').apply(self.preprocess_text)\n",
    "        self.hazard_category_data['title'] = self.hazard_category_data['title'].fillna('').apply(self.preprocess_text)\n",
    "        print(\"✔ Preprocessed title columns successfully.\")\n",
    "\n",
    "        self.vectorize_data()\"\"\"\n",
    "\n",
    "        print(\"✔ Preprocessing completed successfully.\")\n",
    "        return self.data\n",
    "\n",
    "    def save_preprocessed_data(self):\n",
    "        print(\"dimension data- \", self.data.shape)\n",
    "        print(\"dimension product_data- \", self.product_data.shape)\n",
    "        print(\"dimension hazard_data- \", self.hazard_data.shape)\n",
    "        print(\"dimension product_category_data- \", self.product_category_data.shape)\n",
    "        print(\"dimension hazard_category_data- \", self.hazard_category_data.shape)\n",
    "\n",
    "        self.product_data.to_csv('product_data.csv')\n",
    "        self.hazard_data.to_csv('hazard_data.csv')\n",
    "        self.product_category_data.to_csv('product_category_data.csv')\n",
    "        self.hazard_category_data.to_csv('hazard_category_data.csv')\n",
    "\n",
    "        files.download('product_data.csv')\n",
    "        files.download('hazard_data.csv')\n",
    "        files.download('product_category_data.csv')\n",
    "        files.download('hazard_category_data.csv')\n",
    "\n",
    "        \"\"\"self.product_data.to_csv(f'gs://{bucket_name}/final_preprocessed_product_data.csv', index=False)\n",
    "        self.hazard_data.to_csv(f'gs://{bucket_name}/final_preprocessed_hazard_data.csv', index=False)\n",
    "        self.product_category_data.to_csv(f'gs://{bucket_name}/final_preprocessed_product_category_data.csv', index=False)\n",
    "        self.hazard_category_data.to_csv(f'gs://{bucket_name}/final_preprocessed_hazard_category_data.csv', index=False)\"\"\"\n",
    "\n",
    "        print(\"✔ Final preprocessed data saved to 'final_preprocessed_..._data.csv'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = f'gs://{bucket_name}/incidents_train.csv'\n",
    "    mappings_file = f'gs://{bucket_name}/label_mappings.json'\n",
    "    preprocessor = DataPreprocessorLogisticReg(file_path, mappings_file)\n",
    "    preprocessed_data = preprocessor.preprocess()\n",
    "    preprocessor.save_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vl6ThrPcBhgh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from typing import List\n",
    "import transformers\n",
    "import torch\n",
    "from google.colab import drive\n",
    "import getpass\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import gcsfs\n",
    "from google.colab import files\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# Initialize client\n",
    "client = storage.Client()\n",
    "\n",
    "# Set your bucket name\n",
    "bucket_name = \"<bucket_name>\"\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "login(getpass.getpass('Enter your huggingface API-key:'))\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "llm = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=\"cuda\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

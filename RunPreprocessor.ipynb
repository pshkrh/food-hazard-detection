{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "cFXO79neFUmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8b197nUEJYu"
      },
      "outputs": [],
      "source": [
        "class DataPreprocessorLogisticReg:\n",
        "    def __init__(self, file_path, mappings_file):\n",
        "        self.file_path = file_path\n",
        "        self.data = pd.read_csv(file_path)\n",
        "        self.data = self.data.sample(frac=0.0005, random_state=42)\n",
        "        print(\"✔ Loaded dataset successfully.\")\n",
        "\n",
        "        # Load mappings from the provided JSON file\n",
        "        with fs.open(mappings_file, 'r') as f:\n",
        "            mappings = json.load(f)\n",
        "            self.product_category_indices = mappings['product_category_indices']\n",
        "            self.hazard_category_indices = mappings['hazard_category_indices']\n",
        "            self.product_indices = mappings['product_indices']\n",
        "            self.hazard_indices = mappings['hazard_indices']\n",
        "\n",
        "        # Initialize reverse mappings\n",
        "        self.product_category_labels = {v: k for k, v in self.product_category_indices.items()}\n",
        "        self.hazard_category_labels = {v: k for k, v in self.hazard_category_indices.items()}\n",
        "        self.product_labels = {v: k for k, v in self.product_indices.items()}\n",
        "        self.hazard_labels = {v: k for k, v in self.hazard_indices.items()}\n",
        "\n",
        "        # Initialize the Lemmatizer and Stop Words\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        self.product_data = self.data[['title', 'product']]\n",
        "        self.hazard_data = self.data[['title', 'hazard']]\n",
        "        self.product_category_data = self.data[['title', 'product-category']]\n",
        "        self.hazard_category_data = self.data[['title', 'hazard-category']]\n",
        "\n",
        "    def remove_html_tags(self, text):\n",
        "        return BeautifulSoup(text, \"html.parser\").get_text() if text else \"\"\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        if text is None:\n",
        "            return \"\"\n",
        "        text = self.remove_html_tags(text)\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        text = ' '.join(\n",
        "            [self.lemmatizer.lemmatize(word) for word in text.split() if\n",
        "             word not in self.stop_words]\n",
        "        )\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    def combine_date_columns(self):\n",
        "        if {'year', 'month', 'day'}.issubset(self.data.columns):\n",
        "            self.data['date'] = pd.to_datetime(\n",
        "                self.data[['year', 'month', 'day']])\n",
        "            self.data['date'] = self.data['date'].astype('int64') // 1e9\n",
        "            self.data.drop(['year', 'month', 'day'], axis=1, inplace=True)\n",
        "        print(\"✔ Combined date columns successfully.\")\n",
        "\n",
        "    def encode_country_column(self):\n",
        "        if 'country' in self.data.columns:\n",
        "            # Save the mapping of the numerical index to the country name\n",
        "            self.country_mapping = dict(\n",
        "                enumerate(self.data['country'].factorize()[1]))\n",
        "            self.data['country'] = pd.factorize(self.data['country'])[0]\n",
        "        print(\"✔ Encoded country column successfully.\")\n",
        "\n",
        "    def vectorize_data(self):\n",
        "        # Vectorizer instance\n",
        "        vectorizer = TfidfVectorizer()\n",
        "\n",
        "        # List to store the updated datasets with TF-IDF vectors\n",
        "        updated_datasets = []\n",
        "\n",
        "        for dataset in [self.product_data, self.hazard_data, self.product_category_data, self.hazard_category_data]:\n",
        "            # Fit and transform the title data\n",
        "            tfidf_matrix = vectorizer.fit_transform(dataset['title'])\n",
        "\n",
        "            # Convert the TF-IDF matrix to a DataFrame\n",
        "            tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])])\n",
        "\n",
        "            # Reset index of TF-IDF DataFrame to align with the dataset\n",
        "            tfidf_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            # Concatenate the original dataset with the TF-IDF DataFrame\n",
        "            dataset_with_tfidf = pd.concat([dataset.reset_index(drop=True), tfidf_df], axis=1)\n",
        "\n",
        "            # Append the updated dataset\n",
        "            updated_datasets.append(dataset_with_tfidf)\n",
        "\n",
        "        # Assign the updated datasets back to their respective variables\n",
        "        self.product_data, self.hazard_data, self.product_category_data, self.hazard_category_data = updated_datasets\n",
        "        print(\"✔ Vectorized data using TF-IDF and added columns successfully.\")\n",
        "\n",
        "    def generate_synthetic_data(self, min_samples = 2):\n",
        "        # create llm pipeline:\n",
        "        # Get special tokens for later:\n",
        "        bos_token_id = llm.tokenizer.convert_tokens_to_ids('<|begin_of_text|>')\n",
        "        eos_token_id = llm.tokenizer.convert_tokens_to_ids('<|eot_id|>')\n",
        "        pad_token_id = llm.tokenizer.convert_tokens_to_ids('<|eot_id|>')\n",
        "\n",
        "        balanced_data = []\n",
        "\n",
        "        self.seperated_dataset = [self.product_data, self.hazard_data, self.product_category_data, self.hazard_category_data]\n",
        "\n",
        "        updated_datasets = []  # To store the updated datasets\n",
        "\n",
        "        for i, spec_data in enumerate(self.seperated_dataset):\n",
        "            print(f\"Number {i}\")\n",
        "\n",
        "            column = spec_data.columns[1]\n",
        "            # Find rare classes\n",
        "            class_counts = spec_data[column].value_counts()\n",
        "            rare_classes = class_counts[class_counts < min_samples].index\n",
        "\n",
        "            prompts = []\n",
        "            representative_rows = []\n",
        "            for rare_class in rare_classes:\n",
        "                # Get data points belonging to the rare class\n",
        "                rare_class_data = spec_data[spec_data[column] == rare_class]\n",
        "\n",
        "                # Generate synthetic samples for the rare class\n",
        "                target_count = min_samples - len(rare_class_data)\n",
        "                # Use one representative prompt to generate the required number of samples\n",
        "                if len(rare_class_data) > 0:\n",
        "                    representative_row = rare_class_data.iloc[0]  # Use the first row as a representative example\n",
        "                    prompt = f\"\"\"\n",
        "                      Task: Generate synthetic data for the given data point to balance the dataset.\n",
        "                      Label Name: {column}\n",
        "                      Label Value: {representative_row[column]}\n",
        "                      Title: {representative_row['title']}\n",
        "                      Instructions:\n",
        "                      1. Use the title as a reference to create a synthetic data point.\n",
        "                      3. Ensure the output aligns with the label value.\n",
        "                      Output: Provide a single, concise title.\n",
        "                      \"\"\"\n",
        "                    prompts.append(prompt)\n",
        "                    representative_rows.append(representative_row)\n",
        "\n",
        "            num_created_data = 1\n",
        "            synthetic_samples = []\n",
        "\n",
        "            if prompts:\n",
        "              # Generate synthetic samples in one call\n",
        "                synthetic_samples = llm(\n",
        "                  prompts,\n",
        "                  max_new_tokens=20,\n",
        "                  num_return_sequences=num_created_data,\n",
        "                  pad_token_id=pad_token_id,\n",
        "                  bos_token_id=bos_token_id,\n",
        "                  eos_token_id=eos_token_id,\n",
        "                )\n",
        "\n",
        "            # Add the generated samples to the balanced dataset\n",
        "            for i, sequences in enumerate(synthetic_samples):\n",
        "                for sequence in sequences:\n",
        "                    new_row = representative_rows[i].copy()\n",
        "                    new_row['title'] = sequence['generated_text'].strip()  # Update the text with the generated output\n",
        "                    balanced_data.append(new_row)\n",
        "                    print(new_row)\n",
        "\n",
        "            # Add the synthetic data to the original dataset\n",
        "            if balanced_data:\n",
        "                balanced_df = pd.DataFrame(balanced_data)\n",
        "                spec_data = pd.concat([spec_data, balanced_df], ignore_index=True)\n",
        "\n",
        "            updated_datasets.append(spec_data)\n",
        "\n",
        "        self.product_data, self.hazard_data, self.product_category_data, self.hazard_category_data = updated_datasets\n",
        "        print(\"✔ Balanced data using LLM-generated synthetic data.\")\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.generate_synthetic_data()\n",
        "        self.product_data['title'] = self.product_data['title'].fillna('').apply(self.preprocess_text)\n",
        "        self.hazard_data['title'] = self.hazard_data['title'].fillna('').apply(self.preprocess_text)\n",
        "        self.product_category_data['title'] = self.product_category_data['title'].fillna('').apply(self.preprocess_text)\n",
        "        self.hazard_category_data['title'] = self.hazard_category_data['title'].fillna('').apply(self.preprocess_text)\n",
        "        print(\"✔ Preprocessed title columns successfully.\")\n",
        "\n",
        "        self.vectorize_data()\n",
        "\n",
        "        print(\"✔ Preprocessing completed successfully.\")\n",
        "        return self.data\n",
        "\n",
        "    def save_preprocessed_data(self):\n",
        "        print(\"dimension data- \", self.data.shape)\n",
        "        print(\"dimension product_data- \", self.product_data.shape)\n",
        "        print(\"dimension hazard_data- \", self.hazard_data.shape)\n",
        "        print(\"dimension product_category_data- \", self.product_category_data.shape)\n",
        "        print(\"dimension hazard_category_data- \", self.hazard_category_data.shape)\n",
        "\n",
        "        self.product_data.to_csv('gs://<bucket_name>/final_preprocessed_product_data.csv', index=False)\n",
        "        self.hazard_data.to_csv('gs://<bucket_name>/final_preprocessed_hazard_data.csv', index=False)\n",
        "        self.product_category_data.to_csv('gs://<bucket_name>/final_preprocessed_product_category_data.csv', index=False)\n",
        "        self.hazard_category_data.to_csv('gs://<bucket_name>/final_preprocessed_hazard_category_data.csv', index=False)\n",
        "\n",
        "        print(\"✔ Final preprocessed data saved to 'final_preprocessed_..._data.csv'.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    file_path = 'gs://<bucket_name>/incidents_train.csv'\n",
        "    mappings_file = 'gs://<bucket_name>/label_mappings.json'\n",
        "    preprocessor = DataPreprocessorLogisticReg(file_path, mappings_file)\n",
        "    preprocessed_data = preprocessor.preprocess()\n",
        "    preprocessor.save_preprocessed_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from typing import List\n",
        "import transformers\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import getpass\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "import bitsandbytes as bnb\n",
        "from google.cloud import storage\n",
        "import json\n",
        "import gcsfs\n",
        "\n",
        "fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "# Initialize client\n",
        "client = storage.Client()\n",
        "\n",
        "# Set your bucket name\n",
        "bucket_name = \"<bucket_name>\"\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "login(getpass.getpass('Enter your huggingface API-key:'))\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "llm = transformers.pipeline(\n",
        "            \"text-generation\",\n",
        "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "            device_map=\"auto\"\n",
        "        )"
      ],
      "metadata": {
        "id": "vl6ThrPcBhgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor.product_data.iloc[3].title\n"
      ],
      "metadata": {
        "id": "LCLJGNaJr_vO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}